{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "M8lh__fFiLIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work.\n",
        "- KNN is a supervised learning algorithm used for both classification and regression. It predicts the class of a data point based on the 'K' closest points in the training set, using a distance metric like Euclidean."
      ],
      "metadata": {
        "id": "JcsHOQu-iLQl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between KNN Classification and KNN Regression\n",
        "- KNN Classification: Assigns the most common class among K neighbors.\n",
        "\n",
        "- KNN Regression: Predicts the average (or weighted average) value of K neighbors."
      ],
      "metadata": {
        "id": "zCV-JwediLUB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the role of the distance metric in KNN.\n",
        "- The distance metric (e.g., Euclidean, Manhattan) determines how “closeness” is measured, directly affecting neighbor selection and prediction accuracy."
      ],
      "metadata": {
        "id": "o8Nx0cc2iLWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the Curse of Dimensionality in KNN.\n",
        "- As dimensions increase, data becomes sparse and distance measures become less meaningful, reducing KNN performance."
      ],
      "metadata": {
        "id": "0RZfz7HmiLZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can we choose the best value of K in KNN.\n",
        "- Use techniques like cross-validation to test various K values and pick the one that minimizes error on validation data."
      ],
      "metadata": {
        "id": "9Ffh8QhBiLbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are KD Tree and Ball Tree in KNN.\n",
        "- These are data structures used to speed up nearest neighbor searches:\n",
        "\n",
        "- KD Tree: Efficient for low-dimensional data.\n",
        "\n",
        "- Ball Tree: Better for high-dimensional or irregular data."
      ],
      "metadata": {
        "id": "ImowPZJJiLu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should you use KD Tree vs. Ball Tree.\n",
        "- Use KD Tree for dimensions < 20.\n",
        "\n",
        "- Use Ball Tree for dimensions > 20 or non-uniform data."
      ],
      "metadata": {
        "id": "x4-fIpWhiLxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  What are the disadvantages of KNN.\n",
        "- Slow at prediction time\n",
        "\n",
        "- Sensitive to outliers and irrelevant features\n",
        "\n",
        "- Requires feature scaling\n",
        "\n",
        "- Poor with high-dimensional data"
      ],
      "metadata": {
        "id": "0VvXQbmmiMgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does feature scaling affect KNN.\n",
        "- Since KNN is distance-based, unscaled features can dominate distance computation. Standardization or normalization is essential."
      ],
      "metadata": {
        "id": "9nCAuv_tiMjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is PCA (Principal Component Analysis).\n",
        "- PCA is an unsupervised dimensionality reduction technique that transforms data into new variables (principal components) that retain maximum variance."
      ],
      "metadata": {
        "id": "1wx8K2m3lz98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How does PCA work.\n",
        "- Standardize data\n",
        "\n",
        "- Compute covariance matrix\n",
        "\n",
        "- Find eigenvalues and eigenvectors\n",
        "\n",
        "- Project data onto top components"
      ],
      "metadata": {
        "id": "8qbXYYBxl0M3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the geometric intuition behind PCA.\n",
        "- PCA finds new axes (principal components) that capture maximum data variance by rotating the coordinate system."
      ],
      "metadata": {
        "id": "_Q5yV4RFl0Pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the difference between Feature Selection and Feature Extraction.\n",
        "- Feature Selection: Selects a subset of original features\n",
        "\n",
        "- Feature Extraction (PCA): Creates new features from original ones"
      ],
      "metadata": {
        "id": "Ry-DOzTDl0Sg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are Eigenvalues and Eigenvectors in PCA.\n",
        "- Eigenvalues: Indicate variance explained by components\n",
        "\n",
        "- Eigenvectors: Directions of new axes (principal components)"
      ],
      "metadata": {
        "id": "Q3CU922ml0az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How do you decide the number of components to keep in PCA.\n",
        "- Use explained variance ratio or scree plot; typically keep components that explain ≥ 95% variance."
      ],
      "metadata": {
        "id": "QpC4ppfbn7FW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Can PCA be used for classification.\n",
        "- Indirectly, yes. PCA reduces dimensionality before using classification models like KNN or SVM."
      ],
      "metadata": {
        "id": "xj7DUT_un7PF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the limitations of PCA.\n",
        "- Assumes linear relationships\n",
        "\n",
        "- Sensitive to scaling\n",
        "\n",
        "- Components may lack interpretability"
      ],
      "metadata": {
        "id": "NfLUSAYGn7W_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. How do KNN and PCA complement each other.\n",
        "- PCA reduces dimensions and noise, which can improve KNN accuracy and speed."
      ],
      "metadata": {
        "id": "9KTRlU7Zn7aG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How does KNN handle missing values in a dataset.\n",
        "- KNN can use KNN imputation to fill missing values by averaging values of nearest neighbors."
      ],
      "metadata": {
        "id": "WsJ0E3EPn7jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA).\n",
        "- PCA: Unsupervised, focuses on variance\n",
        "\n",
        "- LDA: Supervised, focuses on maximizing class separation"
      ],
      "metadata": {
        "id": "5naGQh90n7mD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a KNN Classifier on the Iris dataset and print model accuracy.\n",
        "- from sklearn.datasets import load_iris\n",
        "- from sklearn.neighbors import KNeighbors Classifier\n",
        "- from sklearn.model_selection import train_test_split\n",
        "- from sklearn.metrics import accuracy_score\n",
        "\n",
        "- data = load_iris()\n",
        "- X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3)\n",
        "- model = KNeighborsClassifier(n_neighbors=3)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Accuracy:\", model.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "4G5J9D-Xp9Rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE).\n",
        "- from sklearn.datasets import make_regression\n",
        "- from sklearn.neighbors import KNeighborsRegressor\n",
        "- from sklearn.metrics import mean_squared_error\n",
        "\n",
        "- X, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
        "- X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "- reg = KNeighborsRegressor(n_neighbors=3)\n",
        "reg.fit(X_train, y_train)\n",
        "- preds = reg.predict(X_test)\n",
        "- print(\"MSE:\", mean_squared_error(y_test, preds))"
      ],
      "metadata": {
        "id": "Jj49W-kip9eV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy.\n",
        "- from sklearn.metrics import accuracy_score\n",
        "\n",
        "- Euclidean\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_iris, y_train_iris)\n",
        "acc_euclidean = accuracy_score(y_test_iris, knn_euclidean.predict(X_test_iris))\n",
        "\n",
        "- Manhattan\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_iris, y_train_iris)\n",
        "acc_manhattan = accuracy_score(y_test_iris, knn_manhattan.predict(X_test_iris))\n",
        "\n",
        "- print(\"Euclidean Accuracy:\", acc_euclidean)\n",
        "- print(\"Manhattan Accuracy:\", acc_manhattan)"
      ],
      "metadata": {
        "id": "APjJ8huwp9mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a KNN Classifier with different values of K and visualize decision boundaried.\n",
        "- from sklearn.datasets import make_classification\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "- Generate 2D classification dataset\n",
        "X_2d, y_2d = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, random_state=42)\n",
        "X_train2d, X_test2d, y_train2d, y_test2d = train_test_split(X_2d, y_2d, test_size=0.3, random_state=42)\n",
        "\n",
        "- Plot decision boundaries for different K\n",
        "for k in [1, 3, 5, 7]:\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)\n",
        "    clf.fit(X_train2d, y_train2d)\n",
        "\n",
        "-  Plotting\n",
        "    h = .02\n",
        "    x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1\n",
        "    y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "- plt.figure()\n",
        "    plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA']))\n",
        "    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, edgecolors='k', cmap=ListedColormap(['#FF0000', '#00FF00']))\n",
        "    plt.title(f\"K = {k}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "v_FkWkSFp9pm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Apply Feature Scaling before training a KNN model and compare results with unscaled data.\n",
        "- Without Scaling\n",
        "model_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
        "model_unscaled.fit(X_train_iris, y_train_iris)\n",
        "acc_unscaled = model_unscaled.score(X_test_iris, y_test_iris)\n",
        "\n",
        "- With Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_iris)\n",
        "X_test_scaled = scaler.transform(X_test_iris)\n",
        "\n",
        "- model_scaled = KNeighborsClassifier(n_neighbors=3)\n",
        "model_scaled.fit(X_train_scaled, y_train_iris)\n",
        "acc_scaled = model_scaled.score(X_test_scaled, y_test_iris)\n",
        "\n",
        "- print(\"Accuracy without scaling:\", acc_unscaled)\n",
        "- print(\"Accuracy with scaling:\", acc_scaled)"
      ],
      "metadata": {
        "id": "ZjfNpTcgawxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a PCA model on synthetic data and print the explained variance ratio for each component.\n",
        "- from sklearn.decomposition import PCA\n",
        "\n",
        "- pca = PCA()\n",
        "- pca.fit(X_iris)  # or use synthetic data if preferred\n",
        "- print(\"Explained Variance Ratio:\")\n",
        "- print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "2u8ZOa1gaw5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA.\n",
        "- PCA applied\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_iris)\n",
        "X_test_pca = pca.transform(X_test_iris)\n",
        "\n",
        "- knn_pca = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_pca.fit(X_train_pca, y_train_iris)\n",
        "acc_with_pca = knn_pca.score(X_test_pca, y_test_iris)\n",
        "\n",
        "- Original data\n",
        "knn_orig = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_orig.fit(X_train_iris, y_train_iris)\n",
        "acc_without_pca = knn_orig.score(X_test_iris, y_test_iris)\n",
        "\n",
        "- print(\"Accuracy without PCA:\", acc_without_pca)\n",
        "- print(\"Accuracy with PCA:\", acc_with_pca)"
      ],
      "metadata": {
        "id": "pjvGSYVGaxA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV.\n",
        "- param_grid = {'n_neighbors': range(1, 20)}\n",
        "- grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "- grid_search.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "- print(\"Best Parameters:\", - grid_search.best_params_)\n",
        "- print(\"Best Accuracy:\", grid_search.best_score_)"
      ],
      "metadata": {
        "id": "2aM_mN7MaxHA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a KNN Classifier and check the number of misclassified samples.\n",
        "- y_pred = knn_iris.predict(X_test_iris)\n",
        "- misclassified = (y_pred != y_test_iris).sum()\n",
        "- print(\"Number of misclassified samples:\", misclassified)"
      ],
      "metadata": {
        "id": "_aNfmE0UaxOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Train a PCA model and visualize the cumulative explained variance.\n",
        "- pca = PCA().fit(X_iris)\n",
        "- plt.figure(figsize=(8, 4))\n",
        "- plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
        "- plt.xlabel('Number of Components')\n",
        "- plt.ylabel('Cumulative Explained Variance')\n",
        "- plt.title('PCA - Cumulative Explained Variance')\n",
        "- plt.grid(True)\n",
        "- plt.show()"
      ],
      "metadata": {
        "id": "CaIwOewTbvNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare\n",
        "accuracy.\n",
        "- Uniform weights\n",
        "- knn_uniform = KNeighborsClassifier(weights='uniform')\n",
        "- knn_uniform.fit(X_train_iris, y_train_iris)\n",
        "- acc_uniform = - knn_uniform.score(X_test_iris, y_test_iris)\n",
        "\n",
        "- Distance-based weights\n",
        "- knn_distance = KNeighborsClassifier(weights='distance')\n",
        "- knn_distance.fit(X_train_iris, y_train_iris)\n",
        "- acc_distance = knn_distance.score(X_test_iris, y_test_iris)\n",
        "\n",
        "- print(\"Accuracy with uniform weights:\", acc_uniform)\n",
        "- print(\"Accuracy with distance weights:\", acc_distance)"
      ],
      "metadata": {
        "id": "Yx2DqEB5bvUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a KNN Regressor and analyze the effect of different K values on performance.\n",
        "- for k in [1, 3, 5, 7, 9]:\n",
        "- reg = KNeighborsRegressor(n_neighbors=k)\n",
        "- reg.fit(X_train_syn, y_train_syn)\n",
        "- pred = reg.predict(X_test_syn)\n",
        "- mse = mean_squared_error(y_test_syn, pred)\n",
        "- print(f\"K={k}, MSE={mse:.2f}\")"
      ],
      "metadata": {
        "id": "LLBBS6nkb9Fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Implement KNN Imputation for handling missing values in a dataset.\n",
        "- from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "- Introduce some missing values in iris\n",
        "- X_iris_missing = X_iris.copy()\n",
        "- X_iris_missing[0][0] = np.nan\n",
        "- X_iris_missing[10][2] = np.nan\n",
        "\n",
        "- imputer = KNNImputer(n_neighbors=3)\n",
        "- X_iris_imputed = imputer.fit_transform(X_iris_missing)\n",
        "\n",
        "- print(\"Before Imputation:\", X_iris_missing[:12])\n",
        "- print(\"After Imputation:\", X_iris_imputed[:12])"
      ],
      "metadata": {
        "id": "Vk28WtW4b9MQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a PCA model and visualize the data projection onto the first two principal components.\n",
        "- pca = PCA(n_components=2)\n",
        "- X_pca = pca.fit_transform(X_iris)\n",
        "\n",
        "- plt.figure(figsize=(8, 5))\n",
        "- plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_iris, cmap='viridis', edgecolor='k')\n",
        "- plt.xlabel('PC1')\n",
        "- plt.ylabel('PC2')\n",
        "- plt.title('Projection onto First Two Principal Components')\n",
        "- plt.colorbar()\n",
        "- plt.show()"
      ],
      "metadata": {
        "id": "g2oWf_3Qb9Rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "35.Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance.\n",
        "- KD Tree\n",
        "knn_kd = KNeighborsClassifier(algorithm='kd_tree')\n",
        "- knn_kd.fit(X_train_iris, y_train_iris)\n",
        "- acc_kd = knn_kd.score(X_test_iris, y_test_iris)\n",
        "\n",
        "- Ball Tree\n",
        "knn_ball = KNeighborsClassifier(algorithm='ball_tree')\n",
        "- knn_ball.fit(X_train_iris, y_train_iris)\n",
        "- acc_ball = knn_ball.score(X_test_iris, y_test_iris)\n",
        "\n",
        "- print(\"Accuracy using KD Tree:\", acc_kd)\n",
        "- print(\"Accuracy using Ball Tree:\", acc_ball)"
      ],
      "metadata": {
        "id": "OlnCU9Zwb9Ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot.\n",
        "- Create high-dimensional data\n",
        "- from sklearn.datasets import make_classification\n",
        "- X_hd, _ = make_classification(n_samples=100, n_features=20, random_state=42)\n",
        "\n",
        "- pca_hd = PCA()\n",
        "- pca_hd.fit(X_hd)\n",
        "\n",
        "- Scree Plot\n",
        "- plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(pca_hd.explained_variance_ratio_) + 1), pca_hd.explained_variance_ratio_, marker='o')\n",
        "- plt.xlabel('Principal Component')\n",
        "- plt.ylabel('Explained Variance Ratio')\n",
        "- plt.title('Scree Plot for High-Dimensional Data')\n",
        "- plt.grid(True)\n",
        "- plt.show()"
      ],
      "metadata": {
        "id": "snSQk0e7bvab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score.\n",
        "- y_pred_knn = knn_iris.predict(X_test_iris)\n",
        "\n",
        "- precision = precision_score(y_test_iris, y_pred_knn, average='macro')\n",
        "- recall = recall_score(y_test_iris, y_pred_knn, average='macro')\n",
        "- f1 = f1_score(y_test_iris, y_pred_knn, average='macro')\n",
        "\n",
        "- print(\"Precision:\", precision)\n",
        "- print(\"Recall:\", recall)\n",
        "- print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "u-W9sWXDbvhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a PCA model and analyze the effect of different numbers of components on accuracy.\n",
        "- for n in range(1, 5):\n",
        "- pca = PCA(n_components=n)\n",
        "- X_train_pca = pca.fit_transform(X_train_iris)\n",
        "- X_test_pca = pca.transform(X_test_iris)\n",
        "    \n",
        "- knn = KNeighborsClassifier(n_neighbors=3)\n",
        "- knn.fit(X_train_pca, y_train_iris)\n",
        "- acc = knn.score(X_test_pca, y_test_iris)\n",
        "    \n",
        "- print(f\"PCA components: {n}, Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "wfJdK34FbvoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a KNN Classifier with different leaf_size values and compare accuracy.\n",
        "- for leaf in [10, 20, 30, 40, 50]:\n",
        "- knn = KNeighborsClassifier(n_neighbors=3, leaf_size=leaf)\n",
        "- knn.fit(X_train_iris, y_train_iris)\n",
        "- acc = knn.score(X_test_iris, y_test_iris)\n",
        "- print(f\"Leaf Size: {leaf}, Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "ZCwa85GQ-sJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a PCA model and visualize how data points are transformed before and after PCA.\n",
        "- Before PCA\n",
        "- plt.figure(figsize=(12, 5))\n",
        "- plt.subplot(1, 2, 1)\n",
        "- plt.scatter(X_iris[:, 0], X_iris[:, 1], c=y_iris, cmap='viridis', edgecolor='k')\n",
        "- plt.title('Original Data (First 2 Features)')\n",
        "- plt.xlabel('Feature 1')\n",
        "- plt.ylabel('Feature 2')\n",
        "\n",
        "- After PCA\n",
        "- X_pca = PCA(n_components=2).fit_transform(X_iris)\n",
        "- plt.subplot(1, 2, 2)\n",
        "- plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_iris, cmap='viridis', edgecolor='k')\n",
        "- plt.title('Data After PCA (2 Components)')\n",
        "- plt.xlabel('PC1')\n",
        "- plt.ylabel('PC2')\n",
        "\n",
        "- plt.tight_layout()\n",
        "- plt.show()"
      ],
      "metadata": {
        "id": "LA1JvXvH-sSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report.\n",
        "- from sklearn.datasets import load_wine\n",
        "- from sklearn.metrics import classification_report\n",
        "\n",
        "- Load wine dataset\n",
        "- wine = load_wine()\n",
        "- X_wine, y_wine = wine.data, wine.target\n",
        "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(X_wine, y_wine, test_size=0_"
      ],
      "metadata": {
        "id": "DRuvq-fA-sZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error.\n",
        "- for metric in ['euclidean', 'manhattan']:\n",
        "- knn_reg = KNeighborsRegressor(n_neighbors=3, metric=metric)\n",
        "- knn_reg.fit(X_train_syn, y_train_syn)\n",
        "- y_pred = knn_reg.predict(X_test_syn)\n",
        "- mse = mean_squared_error(y_test_syn, y_pred)\n",
        "- print(f\"Distance Metric: {metric}, MSE: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "tGZwXhKa-sf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a KNN Classifier and evaluate using ROC-AUC score.\n",
        "- from sklearn.preprocessing import label_binarize\n",
        "- from sklearn.metrics import roc_auc_score\n",
        "\n",
        "- Binarize for multiclass ROC AUC\n",
        "y_test_bin = label_binarize(y_test_iris, classes=[0, 1, 2])\n",
        "- y_pred_prob = knn_iris.predict_proba(X_test_iris)\n",
        "\n",
        "- roc_auc = roc_auc_score(y_test_bin, y_pred_prob, average='macro', multi_class='ovr')\n",
        "- print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "id": "7famLo3QJTlL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a PCA model and visualize the variance captured by each principal component.\n",
        "- pca = PCA().fit(X_iris)\n",
        "- plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),  pca.explained_variance_ratio_)\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Variance Captured by Each PCA Component')\n",
        "- plt.show()"
      ],
      "metadata": {
        "id": "J4xfzm8ZJTsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "45.  Train a KNN Classifier and perform feature selection before training.\n",
        "- from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "- Select top 2 features\n",
        "-selector = SelectKBest(score_func=f_classif, k=2)\n",
        "- X_selected = selector.fit_transform(X_iris, y_iris)\n",
        "X_train_sel, X_test_sel, y_train_sel, y_test_sel = train_test_split(X_selected, y_iris, test_size=0.3, random_state=42)\n",
        "\n",
        "- knn_fs = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_fs.fit(X_train_sel, y_train_sel)\n",
        "acc_fs = knn_fs.score(X_test_sel, y_test_sel)\n",
        "- print(\"Accuracy after Feature Selection:\", acc_fs)"
      ],
      "metadata": {
        "id": "wvY2aRvEJTx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Train a PCA model and visualize the data reconstruction error after reducing dimensions.\n",
        "- Reduce to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "- X_reduced = pca.fit_transform(X_iris)\n",
        "\n",
        "- Reconstruct original data\n",
        "X_reconstructed = pca.inverse_transfor"
      ],
      "metadata": {
        "id": "afBgoug5JT4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. Train a KNN Classifier and visualize the decision boundary.\n",
        "- 2D dataset\n",
        "- from matplotlib.colors import ListedColormap\n",
        "- X_vis, y_vis = make_classification(n_samples=100, n_features=2, n_redundant=0, random_state=42)\n",
        "- X_train_v, X_test_v, y_train_v, y_test_v = train_test_split(X_vis, y_vis, test_size=0.3, random_state=42)\n",
        "\n",
        "- knn_vis = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_vis.fit(X_train_v, y_train_v)\n",
        "\n",
        "- Decision boundary\n",
        "h = .02\n",
        "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
        "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = knn_vis.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "- plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA']))\n",
        "- plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, edgecolor='k', cmap=ListedColormap(['#FF0000', '#00FF00']))\n",
        "- plt.title(\"KNN Decision Boundary\")\n",
        "- plt.xlabel(\"Feature 1\")\n",
        "- plt.ylabel(\"Feature 2\")\n",
        "- plt.show()"
      ],
      "metadata": {
        "id": "6l4QRRsfJUAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. Train a PCA model and analyze the effect of different numbers of components on data variance.\n",
        "- Try various component counts and track explained variance\n",
        "- components = [1, 2, 3, 4]\n",
        "for n in components:\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(X_iris)\n",
        "    total_var = np.sum(pca.explained_variance_ratio_)\n",
        "- print(f\"Components: {n}, Total Explained - Variance: {total_var:.2f}\")"
      ],
      "metadata": {
        "id": "ueAAj56Y-snh"
      }
    }
  ]
}